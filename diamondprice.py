# -*- coding: utf-8 -*-
"""diamondprice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K1LAGtCe74pA4Oi8qJKavoLJFbWCmQ5k
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import zscore
from sklearn.impute import SimpleImputer

df = pd.read_csv('diamonds.csv')

print("Missing values per column:\n", df.isnull().sum())
print("\nDataset Summary:\n", df.describe())

# correlations with heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# relationships
sns.pairplot(df[['carat', 'depth', 'table', 'price']])
plt.show()

# remove rows with feature more than 3 standard deviations from mean
num_cols = ['carat', 'depth', 'table', 'x', 'y', 'z']
df = df[(np.abs(zscore(df[num_cols])) < 3).all(axis=1)]

# columns are numerical and categorical
cat_feats = ['cut', 'color', 'clarity']
num_feats = ['carat', 'depth', 'table', 'x', 'y', 'z']

# imputing missing values using mean, then standardize
num_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# imputing missing values using most frequent value, then one-hot encode
cat_pipe = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# combine pipelines into preprocessor
preprocessor = ColumnTransformer(transformers=[
    ('num', num_pipe, num_feats),
    ('cat', cat_pipe, cat_feats)
])

# dictionary of models to compare
models = {
    'Linear Regression': LinearRegression(),
    'Polynomial Regression': Pipeline([
        ('poly', PolynomialFeatures(degree=2)),
        ('regressor', LinearRegression())
    ]),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.01),
    'ElasticNet': ElasticNet(alpha=0.01, l1_ratio=0.5)
}

X = df.drop('price', axis=1)
y = df['price']

# split training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

results = []

# evaluate each model using cross-validation and test set metrics
for name, model in models.items():
    # create pipeline that first preprocesses the data then applies the model
    pipe = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    # cross-validation using R^2 scoring metric
    cv_score = cross_val_score(pipe, X_train, y_train, cv=5, scoring='r2').mean()

    # Fit training set and predict on test set
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    # Append results
    results.append({
        'Model': name,
        'CV R2 Mean': cv_score,
        'Test R2': r2_score(y_test, y_pred),
        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_pred))
    })

# compare model performances
results_df = pd.DataFrame(results)
print("\nModel Comparison:\n", results_df.sort_values(by='Test R2', ascending=False))

# fit Linear Regression model
lr_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', LinearRegression())
])
lr_pipeline.fit(X_train, y_train)

# retrieve one-hot encoded feature names from the categorical transformer
cat_feature_names = lr_pipeline.named_steps['preprocessor']\
    .named_transformers_['cat']\
    .named_steps['encoder']\
    .get_feature_names_out(cat_feats)
all_features = num_feats + list(cat_feature_names)

# dataFrame for the coefficients
coeffs = pd.DataFrame({
    'Feature': all_features,
    'Coefficient': lr_pipeline.named_steps['model'].coef_
}).sort_values(by='Coefficient', ascending=False)

print("\nTop 10 Linear Regression Coefficients:")
print(coeffs.head(10))